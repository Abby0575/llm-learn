# llm-learn
# 这个博客旨在记录和分享大模型的学习过程和资源
一、Transformer架构
1. 注意力机制  
(1) 注意力机制的基本原理和计算公式  
b站：范仁义-AI编程：https://www.bilibili.com/video/BV1nL4y1j7hA?spm_id_from=333.788.videopod.sections&vd_source=6ae59bbd574563290e3b85bc1e33e572  
词向量：四种词向量表示方式：  
  1. Zero-Hot
  2. Word2Vec
  3. WordPiece
  4. BFE  
2. Bert模型

